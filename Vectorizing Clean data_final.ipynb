{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the cleaned data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>clean_question1</th>\n",
       "      <th>clean_question2</th>\n",
       "      <th>lemmatized_question1</th>\n",
       "      <th>lemmatized_question2</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government steal kohinoor ...</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>increase speed internet connection use vpn</td>\n",
       "      <td>internet speed increase hack dns</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder 23 power 24 divided 24 23</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder 23 power 24 divide 24 23</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2  is_duplicate  \\\n",
       "0   0     1     2             0   \n",
       "1   1     3     4             0   \n",
       "2   2     5     6             0   \n",
       "3   3     7     8             0   \n",
       "4   4     9    10             0   \n",
       "\n",
       "                                     clean_question1  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor koh noor diamond   \n",
       "2       increase speed internet connection using vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                                     clean_question2  \\\n",
       "0                step step guide invest share market   \n",
       "1  would happen indian government stole kohinoor ...   \n",
       "2               internet speed increased hacking dns   \n",
       "3           find remainder 23 power 24 divided 24 23   \n",
       "4                      fish would survive salt water   \n",
       "\n",
       "                                lemmatized_question1  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor koh noor diamond   \n",
       "2         increase speed internet connection use vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                                lemmatized_question2  len_q1  len_q2  \n",
       "0                step step guide invest share market      41      35  \n",
       "1  would happen indian government steal kohinoor ...      31      67  \n",
       "2                   internet speed increase hack dns      42      32  \n",
       "3            find remainder 23 power 24 divide 24 23      21      39  \n",
       "4                      fish would survive salt water      60      29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the whole dataset for training the vectorizer on the whole dataset\n",
    "og_data = pd.read_csv('cleaned_questions.csv')\n",
    "og_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lengthq1</th>\n",
       "      <th>lengthq2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>q1_wordlen</th>\n",
       "      <th>q2_wordlen</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>clean_question1</th>\n",
       "      <th>clean_question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236588</td>\n",
       "      <td>466074</td>\n",
       "      <td>466075</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284623</td>\n",
       "      <td>413904</td>\n",
       "      <td>559402</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>good alternative cut brisket can not find</td>\n",
       "      <td>best wood smoke brisket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37445</td>\n",
       "      <td>74608</td>\n",
       "      <td>74609</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>horror movie jump scare</td>\n",
       "      <td>possible create good horror film without jump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299330</td>\n",
       "      <td>587921</td>\n",
       "      <td>587922</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>ethical take vegetarian v vegan v non vegetari...</td>\n",
       "      <td>non vegetarian date vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204421</td>\n",
       "      <td>403323</td>\n",
       "      <td>403324</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>good tip young biotech enterpreneurs</td>\n",
       "      <td>must young entrepreneur know build company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2  is_duplicate  lengthq1  lengthq2  common_words  \\\n",
       "0  236588  466074  466075             0       120       119            19   \n",
       "1  284623  413904  559402             0        61        39             1   \n",
       "2   37445   74608   74609             0        44        64             3   \n",
       "3  299330  587921  587922             0        76        39             1   \n",
       "4  204421  403323  403324             0        56        63             2   \n",
       "\n",
       "   q1_wordlen  q2_wordlen  word_difference  \\\n",
       "0          22          22                0   \n",
       "1          12           8                4   \n",
       "2           8          12                4   \n",
       "3          12           7                5   \n",
       "4           9          10                1   \n",
       "\n",
       "                                     clean_question1  \\\n",
       "0  good gift foreign visitor bring invite someone...   \n",
       "1          good alternative cut brisket can not find   \n",
       "2                            horror movie jump scare   \n",
       "3  ethical take vegetarian v vegan v non vegetari...   \n",
       "4               good tip young biotech enterpreneurs   \n",
       "\n",
       "                                     clean_question2  \n",
       "0  good gift foreign visitor bring invite someone...  \n",
       "1                            best wood smoke brisket  \n",
       "2  possible create good horror film without jump ...  \n",
       "3                     non vegetarian date vegetarian  \n",
       "4         must young entrepreneur know build company  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading both the datasets based on nltk and spacy\n",
    "nltk = pd.read_csv('cleaned_questions_nltk.csv')\n",
    "nltk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lengthq1</th>\n",
       "      <th>lengthq2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>q1_wordlen</th>\n",
       "      <th>q2_wordlen</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>clean_question1</th>\n",
       "      <th>clean_question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236588</td>\n",
       "      <td>466074</td>\n",
       "      <td>466075</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284623</td>\n",
       "      <td>413904</td>\n",
       "      <td>559402</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>good alternative cut brisket can not find</td>\n",
       "      <td>good wood smoke brisket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37445</td>\n",
       "      <td>74608</td>\n",
       "      <td>74609</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>horror movie jump scare</td>\n",
       "      <td>possible create good horror film without jump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299330</td>\n",
       "      <td>587921</td>\n",
       "      <td>587922</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>ethical take vegetarian vs vegan vs non vegeta...</td>\n",
       "      <td>non vegetarian date vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204421</td>\n",
       "      <td>403323</td>\n",
       "      <td>403324</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>good tip young biotech enterpreneur</td>\n",
       "      <td>must young entrepreneur know build company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2  is_duplicate  lengthq1  lengthq2  common_words  \\\n",
       "0  236588  466074  466075             0       120       119            19   \n",
       "1  284623  413904  559402             0        61        39             1   \n",
       "2   37445   74608   74609             0        44        64             3   \n",
       "3  299330  587921  587922             0        76        39             1   \n",
       "4  204421  403323  403324             0        56        63             2   \n",
       "\n",
       "   q1_wordlen  q2_wordlen  word_difference  \\\n",
       "0          22          22                0   \n",
       "1          12           8                4   \n",
       "2           8          12                4   \n",
       "3          12           7                5   \n",
       "4           9          10                1   \n",
       "\n",
       "                                     clean_question1  \\\n",
       "0  good gift foreign visitor bring invite someone...   \n",
       "1          good alternative cut brisket can not find   \n",
       "2                            horror movie jump scare   \n",
       "3  ethical take vegetarian vs vegan vs non vegeta...   \n",
       "4                good tip young biotech enterpreneur   \n",
       "\n",
       "                                     clean_question2  \n",
       "0  good gift foreign visitor bring invite someone...  \n",
       "1                            good wood smoke brisket  \n",
       "2  possible create good horror film without jump ...  \n",
       "3                     non vegetarian date vegetarian  \n",
       "4         must young entrepreneur know build company  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy = pd.read_csv('cleaned_questions_spacy.csv')\n",
    "spacy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Vectorising using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 20577), (10000, 20577), (10000, 20577), (10000, 20577))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing the 'clean_question1' and 'clean_question2' columns of the og_data, spacy and nltk datasets\n",
    "\n",
    "# Setting up min_df, max_df and other params to ensure matrix is not that sparse\n",
    "tfidf = TfidfVectorizer(min_df=10, max_df=0.7)\n",
    "\n",
    "# Removing nulls before fitting and transforming as TfidfVectorizer does not handle nulls\n",
    "og_data['clean_question1'].fillna(\"\", inplace=True)\n",
    "og_data['clean_question2'].fillna(\"\", inplace=True)\n",
    "nltk['clean_question1'].fillna(\"\", inplace=True)\n",
    "nltk['clean_question2'].fillna(\"\", inplace=True)\n",
    "spacy['clean_question1'].fillna(\"\", inplace=True)\n",
    "spacy['clean_question2'].fillna(\"\", inplace=True)\n",
    "\n",
    "tfidf.fit(pd.concat([og_data['clean_question1'], og_data['clean_question2']]))\n",
    "\n",
    "tfidf_nltk_q1 = tfidf.transform(nltk['clean_question1'])\n",
    "tfidf_nltk_q2 = tfidf.transform(nltk['clean_question2'])\n",
    "\n",
    "tfidf_spacy_q1 = tfidf.transform(spacy['clean_question1'])\n",
    "tfidf_spacy_q2 = tfidf.transform(spacy['clean_question2'])\n",
    "\n",
    "tfidf_nltk_q1.shape, tfidf_nltk_q2.shape, tfidf_spacy_q1.shape, tfidf_spacy_q2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after using min_df and max_df, the matrix is still very sparse. This is because the dataset is very large and the number of unique words is also very large. This is why we will use Word2Vec to convert the text into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vectorising using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 100), (10000, 100), (10000, 100), (10000, 100))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining tokens from q1 and q2 of og_data for training the Word2Vec model\n",
    "combined_tokens = og_data['clean_question1'].apply(word_tokenize).tolist() + og_data['clean_question2'].apply(word_tokenize).tolist()\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(combined_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(combined_tokens, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "# Adjusting the get_average_word2vec function to work with the vector model and tokens\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=100):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "# Adjusting the get_word2vec_embeddings function to handle both q1 and q2\n",
    "def get_word2vec_embeddings(vectors, clean_questions):\n",
    "    tokens = clean_questions.apply(word_tokenize)\n",
    "    embeddings = tokens.apply(lambda x: get_average_word2vec(x, vectors))\n",
    "    return list(embeddings)\n",
    "\n",
    "# Applying the function to get the Word2Vec embeddings for both questions in nltk and spacy datasets\n",
    "nltk_q1_embeddings = get_word2vec_embeddings(model.wv, nltk['clean_question1'])\n",
    "nltk_q2_embeddings = get_word2vec_embeddings(model.wv, nltk['clean_question2'])\n",
    "spacy_q1_embeddings = get_word2vec_embeddings(model.wv, spacy['clean_question1'])\n",
    "spacy_q2_embeddings = get_word2vec_embeddings(model.wv, spacy['clean_question2'])\n",
    "\n",
    "# Verifying the shape of the embeddings\n",
    "np.array(nltk_q1_embeddings).shape, np.array(nltk_q2_embeddings).shape, np.array(spacy_q1_embeddings).shape, np.array(spacy_q2_embeddings).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating the word2vec embeddings for q1 and q2 of both nltk and spacy datasets and adding them to the original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the embeddings to dataframes\n",
    "temp1 = pd.DataFrame(nltk_q1_embeddings, index=nltk.index)\n",
    "temp2 = pd.DataFrame(nltk_q2_embeddings, index=nltk.index)\n",
    "\n",
    "# Concatenating the embeddings to the nltk dataframe\n",
    "nltk = pd.concat([nltk, temp1, temp2], axis=1)\n",
    "\n",
    "# Repeating the process for the spacy dataframe\n",
    "temp1 = pd.DataFrame(spacy_q1_embeddings, index=spacy.index)\n",
    "temp2 = pd.DataFrame(spacy_q2_embeddings, index=spacy.index)\n",
    "\n",
    "spacy = pd.concat([spacy, temp1, temp2], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframes to csv for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.to_csv('nltk_embeddings.csv', index=False)\n",
    "spacy.to_csv('spacy_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enterpriseml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
