{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the cleaned data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omsan\\anaconda3\\envs\\enterpriseml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import scipy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from joblib import dump\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>clean_question1</th>\n",
       "      <th>clean_question2</th>\n",
       "      <th>lemmatized_question1</th>\n",
       "      <th>lemmatized_question2</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government steal kohinoor ...</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>increase speed internet connection use vpn</td>\n",
       "      <td>internet speed increase hack dns</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder 23 power 24 divided 24 23</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder 23 power 24 divide 24 23</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2  is_duplicate  \\\n",
       "0   0     1     2             0   \n",
       "1   1     3     4             0   \n",
       "2   2     5     6             0   \n",
       "3   3     7     8             0   \n",
       "4   4     9    10             0   \n",
       "\n",
       "                                     clean_question1  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor koh noor diamond   \n",
       "2       increase speed internet connection using vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                                     clean_question2  \\\n",
       "0                step step guide invest share market   \n",
       "1  would happen indian government stole kohinoor ...   \n",
       "2               internet speed increased hacking dns   \n",
       "3           find remainder 23 power 24 divided 24 23   \n",
       "4                      fish would survive salt water   \n",
       "\n",
       "                                lemmatized_question1  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor koh noor diamond   \n",
       "2         increase speed internet connection use vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                                lemmatized_question2  len_q1  len_q2  \n",
       "0                step step guide invest share market      41      35  \n",
       "1  would happen indian government steal kohinoor ...      31      67  \n",
       "2                   internet speed increase hack dns      42      32  \n",
       "3            find remainder 23 power 24 divide 24 23      21      39  \n",
       "4                      fish would survive salt water      60      29  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the whole dataset for training the vectorizer on the whole dataset\n",
    "og_data = pd.read_csv('cleaned_questions.csv')\n",
    "og_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lengthq1</th>\n",
       "      <th>lengthq2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>q1_wordlen</th>\n",
       "      <th>q2_wordlen</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>clean_question1</th>\n",
       "      <th>clean_question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236588</td>\n",
       "      <td>466074</td>\n",
       "      <td>466075</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284623</td>\n",
       "      <td>413904</td>\n",
       "      <td>559402</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>good alternative cut brisket can not find</td>\n",
       "      <td>best wood smoke brisket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37445</td>\n",
       "      <td>74608</td>\n",
       "      <td>74609</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>horror movie jump scare</td>\n",
       "      <td>possible create good horror film without jump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299330</td>\n",
       "      <td>587921</td>\n",
       "      <td>587922</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>ethical take vegetarian v vegan v non vegetari...</td>\n",
       "      <td>non vegetarian date vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204421</td>\n",
       "      <td>403323</td>\n",
       "      <td>403324</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>good tip young biotech enterpreneurs</td>\n",
       "      <td>must young entrepreneur know build company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2  is_duplicate  lengthq1  lengthq2  common_words  \\\n",
       "0  236588  466074  466075             0       120       119            19   \n",
       "1  284623  413904  559402             0        61        39             1   \n",
       "2   37445   74608   74609             0        44        64             3   \n",
       "3  299330  587921  587922             0        76        39             1   \n",
       "4  204421  403323  403324             0        56        63             2   \n",
       "\n",
       "   q1_wordlen  q2_wordlen  word_difference  \\\n",
       "0          22          22                0   \n",
       "1          12           8                4   \n",
       "2           8          12                4   \n",
       "3          12           7                5   \n",
       "4           9          10                1   \n",
       "\n",
       "                                     clean_question1  \\\n",
       "0  good gift foreign visitor bring invite someone...   \n",
       "1          good alternative cut brisket can not find   \n",
       "2                            horror movie jump scare   \n",
       "3  ethical take vegetarian v vegan v non vegetari...   \n",
       "4               good tip young biotech enterpreneurs   \n",
       "\n",
       "                                     clean_question2  \n",
       "0  good gift foreign visitor bring invite someone...  \n",
       "1                            best wood smoke brisket  \n",
       "2  possible create good horror film without jump ...  \n",
       "3                     non vegetarian date vegetarian  \n",
       "4         must young entrepreneur know build company  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading both the datasets based on nltk and spacy\n",
    "nltk_data = pd.read_csv('cleaned_questions_nltk.csv')\n",
    "nltk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lengthq1</th>\n",
       "      <th>lengthq2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>q1_wordlen</th>\n",
       "      <th>q2_wordlen</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>clean_question1</th>\n",
       "      <th>clean_question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236588</td>\n",
       "      <td>466074</td>\n",
       "      <td>466075</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "      <td>good gift foreign visitor bring invite someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284623</td>\n",
       "      <td>413904</td>\n",
       "      <td>559402</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>good alternative cut brisket can not find</td>\n",
       "      <td>good wood smoke brisket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37445</td>\n",
       "      <td>74608</td>\n",
       "      <td>74609</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>horror movie jump scare</td>\n",
       "      <td>possible create good horror film without jump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299330</td>\n",
       "      <td>587921</td>\n",
       "      <td>587922</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>ethical take vegetarian vs vegan vs non vegeta...</td>\n",
       "      <td>non vegetarian date vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204421</td>\n",
       "      <td>403323</td>\n",
       "      <td>403324</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>good tip young biotech enterpreneur</td>\n",
       "      <td>must young entrepreneur know build company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2  is_duplicate  lengthq1  lengthq2  common_words  \\\n",
       "0  236588  466074  466075             0       120       119            19   \n",
       "1  284623  413904  559402             0        61        39             1   \n",
       "2   37445   74608   74609             0        44        64             3   \n",
       "3  299330  587921  587922             0        76        39             1   \n",
       "4  204421  403323  403324             0        56        63             2   \n",
       "\n",
       "   q1_wordlen  q2_wordlen  word_difference  \\\n",
       "0          22          22                0   \n",
       "1          12           8                4   \n",
       "2           8          12                4   \n",
       "3          12           7                5   \n",
       "4           9          10                1   \n",
       "\n",
       "                                     clean_question1  \\\n",
       "0  good gift foreign visitor bring invite someone...   \n",
       "1          good alternative cut brisket can not find   \n",
       "2                            horror movie jump scare   \n",
       "3  ethical take vegetarian vs vegan vs non vegeta...   \n",
       "4                good tip young biotech enterpreneur   \n",
       "\n",
       "                                     clean_question2  \n",
       "0  good gift foreign visitor bring invite someone...  \n",
       "1                            good wood smoke brisket  \n",
       "2  possible create good horror film without jump ...  \n",
       "3                     non vegetarian date vegetarian  \n",
       "4         must young entrepreneur know build company  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_data = pd.read_csv('cleaned_questions_spacy.csv')\n",
    "spacy_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Vectorising using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data['clean_question1'].fillna(\"\", inplace=True)\n",
    "    data['clean_question2'].fillna(\"\", inplace=True)\n",
    "    return data\n",
    "\n",
    "def vectorize_questions(data, tfidf_vectorizer):\n",
    "    tfidf_q1 = tfidf_vectorizer.transform(data['clean_question1'])\n",
    "    tfidf_q2 = tfidf_vectorizer.transform(data['clean_question2'])\n",
    "    return tfidf_q1, tfidf_q2\n",
    "\n",
    "# # Preprocess OG data\n",
    "# og_data_preprocessed = preprocess_data(og_data)\n",
    "\n",
    "# # Train TF-IDF Vectorizer on OG data\n",
    "# tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.5)\n",
    "# tfidf_vectorizer.fit(pd.concat([og_data_preprocessed['clean_question1'], og_data_preprocessed['clean_question2']]))\n",
    "\n",
    "# # Vectorize OG data questions\n",
    "# tfidf_og_q1, tfidf_og_q2 = vectorize_questions(og_data_preprocessed, tfidf_vectorizer)\n",
    "\n",
    "# # Combine and train Truncated SVD on the vectorized OG data\n",
    "# svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "# combined_tfidf_og = scipy.sparse.vstack((tfidf_og_q1, tfidf_og_q2))\n",
    "# svd.fit(combined_tfidf_og)\n",
    "\n",
    "\n",
    "# # Save the TF-IDF vectorizer\n",
    "# dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# # Save the Truncated SVD model\n",
    "# dump(svd, 'svd_model.joblib')\n",
    "\n",
    "\n",
    "# # Load the TF-IDF vectorizer\n",
    "tfidf_vectorizer = load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# # Load the Truncated SVD model\n",
    "svd = load('svd_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(tfidf_q1, tfidf_q2, svd):\n",
    "    tfidf_q1_reduced = svd.transform(tfidf_q1)\n",
    "    tfidf_q2_reduced = svd.transform(tfidf_q2)\n",
    "    return tfidf_q1_reduced, tfidf_q2_reduced\n",
    "\n",
    "\n",
    "def calculate_squared_differences(tfidf_q1_reduced, tfidf_q2_reduced):\n",
    "    squared_differences = np.square(tfidf_q1_reduced - tfidf_q2_reduced)\n",
    "    return squared_differences\n",
    "\n",
    "\n",
    "\n",
    "def process_pipeline(data, tfidf_vectorizer, svd):\n",
    "    # Step 1: Preprocess the data\n",
    "    data = preprocess_data(data)\n",
    "    \n",
    "    # Step 2: Vectorize the questions with pre-trained TF-IDF vectorizer\n",
    "    tfidf_q1, tfidf_q2 = vectorize_questions(data, tfidf_vectorizer)\n",
    "    \n",
    "    # Step 3: Reduce the dimensionality with pre-trained Truncated SVD\n",
    "    tfidf_q1_reduced, tfidf_q2_reduced = reduce_dimensionality(tfidf_q1, tfidf_q2, svd)\n",
    "    \n",
    "    # Step 4: Calculate squared differences\n",
    "    squared_differences = calculate_squared_differences(tfidf_q1_reduced, tfidf_q2_reduced)\n",
    "    \n",
    "    return squared_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing the 'nltk' dataset through the pipeline\n",
    "squared_differences_nltk = process_pipeline(nltk_data, tfidf_vectorizer, svd)\n",
    "\n",
    "# Example: Processing the 'spacy' dataset through the pipeline\n",
    "squared_differences_spacy = process_pipeline(spacy_data, tfidf_vectorizer, svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert squared differences to DataFrame\n",
    "squared_differences_nltk_df = pd.DataFrame(squared_differences_nltk)\n",
    "squared_differences_spacy_df = pd.DataFrame(squared_differences_spacy)\n",
    "\n",
    "# Append squared differences to the nltk dataframe\n",
    "nltk_data = pd.concat([nltk_data, squared_differences_nltk_df], axis=1)\n",
    "\n",
    "# Append squared differences to the spacy dataframe\n",
    "spacy_data = pd.concat([spacy_data, squared_differences_spacy_df], axis=1)\n",
    "\n",
    "# Save the nltk dataframe\n",
    "nltk_data.to_csv('nltk_embeddings.csv', index=False)\n",
    "\n",
    "# Save the spacy dataframe\n",
    "spacy_data.to_csv('spacy_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vectorising using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m squared_differences\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m## Getting squared differences for the nltk dataset using BERT embeddings\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m nltk_squared_differences \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pipeline_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Convert the squared differences to a dataframe\u001b[39;00m\n\u001b[0;32m     48\u001b[0m nltk_squared_differences_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(nltk_squared_differences)\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mprocess_pipeline_bert\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     33\u001b[0m data \u001b[38;5;241m=\u001b[39m preprocess_data(data)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Step 2: Get BERT embeddings for the questions\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m q1_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mbert_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_question1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m q2_embeddings \u001b[38;5;241m=\u001b[39m bert_embeddings(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_question2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Step 3: Calculate squared differences\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mbert_embeddings\u001b[1;34m(texts, model_name, max_length, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_texts \u001b[38;5;241m=\u001b[39m texts[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Tokenize and encode the batch\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m encoded_input\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get embeddings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omsan\\anaconda3\\envs\\enterpriseml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2802\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2803\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\omsan\\anaconda3\\envs\\enterpriseml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2861\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2861\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2862\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2863\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2864\u001b[0m     )\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2868\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2870\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "def bert_embeddings(texts, model_name='bert-base-uncased', max_length=128, batch_size=32):\n",
    "    # Load pre-trained model tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "    \n",
    "    # Check if CUDA is available and if not, use CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    embeddings = []\n",
    "\n",
    "    # Process texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize and encode the batch\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        encoded_input = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "        \n",
    "        # Move embeddings to CPU and convert to numpy\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def process_pipeline_bert(data):\n",
    "    # Step 1: Preprocess the data\n",
    "    data = preprocess_data(data)\n",
    "    \n",
    "    # Step 2: Get BERT embeddings for the questions\n",
    "    q1_embeddings = bert_embeddings(data['clean_question1'])\n",
    "    q2_embeddings = bert_embeddings(data['clean_question2'])\n",
    "    \n",
    "    # Step 3: Calculate squared differences\n",
    "    squared_differences = calculate_squared_differences(q1_embeddings, q2_embeddings)\n",
    "    \n",
    "    return squared_differences\n",
    "\n",
    "## Getting squared differences for the nltk dataset using BERT embeddings\n",
    "nltk_squared_differences = process_pipeline_bert(nltk_data)\n",
    "\n",
    "# Convert the squared differences to a dataframe\n",
    "nltk_squared_differences_df = pd.DataFrame(nltk_squared_differences)\n",
    "\n",
    "# Append squared differences to the nltk dataframe\n",
    "nltk_data = pd.concat([nltk_data, nltk_squared_differences_df], axis=1)\n",
    "\n",
    "# Save the nltk dataframe\n",
    "#nltk_data.to_csv('nltk_embeddings_bert.csv', index=False)\n",
    "\n",
    "\n",
    "## Getting squared differences for the spacy dataset using BERT embeddings\n",
    "spacy_squared_differences = process_pipeline_bert(spacy_data)\n",
    "\n",
    "# Convert the squared differences to a dataframe\n",
    "spacy_squared_differences_df = pd.DataFrame(spacy_squared_differences)\n",
    "\n",
    "# Append squared differences to the spacy dataframe\n",
    "spacy_data = pd.concat([spacy_data, spacy_squared_differences_df], axis=1)\n",
    "\n",
    "# Save the spacy dataframe\n",
    "#spacy_data.to_csv('spacy_embeddings_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enterpriseml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
