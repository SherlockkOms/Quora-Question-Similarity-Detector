{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\omsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure all necessary NLTK data is downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the pre-trained models\n",
    "tfidf_vectorizer = load('tfidf_vectorizer.joblib')\n",
    "svd_model = load('svd_model.joblib')\n",
    "gb_model = load('gbmodel.joblib')\n",
    "\n",
    "# Set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_contractions(text):\n",
    "    # Dictionary of English contractions\n",
    "    contractions_dict = {\"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "                         }\n",
    "    # Regular expression for finding contractions\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "#function to handle LaTeX expressions\n",
    "def clean_math_text(text):\n",
    "\n",
    "    replacements = {\n",
    "        # Basic operations and structures\n",
    "        r'\\\\frac\\{(.*?)\\}\\{(.*?)\\}': r'\\1 over \\2',\n",
    "        r'\\\\sqrt\\{(.*?)\\}': r'square root of \\1',\n",
    "        r'\\\\sum_(\\{.*?\\})\\^(\\{.*?\\})': r'sum from \\1 to \\2',\n",
    "        r'\\\\int_(\\{.*?\\})\\^(\\{.*?\\})': r'integral from \\1 to \\2',\n",
    "        r'\\\\log_(\\{.*?\\})\\{(.*?)\\}': r'log base \\1 of \\2',\n",
    "        r'\\\\lim_(\\{.*?\\})': r'limit as \\1',\n",
    "        r'(\\d+)\\^(\\{?\\d+\\}?)': r'\\1 to the power of \\2',\n",
    "        r'\\\\infty': 'infinity',\n",
    "        r'\\\\pm': 'plus or minus',\n",
    "        # Greek letters\n",
    "        r'\\\\alpha': 'alpha', r'\\\\beta': 'beta', r'\\\\gamma': 'gamma',\n",
    "        r'\\\\delta': 'delta', r'\\\\epsilon': 'epsilon', r'\\\\zeta': 'zeta',\n",
    "        r'\\\\eta': 'eta', r'\\\\theta': 'theta', r'\\\\iota': 'iota',\n",
    "        r'\\\\kappa': 'kappa', r'\\\\lambda': 'lambda', r'\\\\mu': 'mu',\n",
    "        r'\\\\nu': 'nu', r'\\\\xi': 'xi', r'\\\\omicron': 'omicron',\n",
    "        r'\\\\pi': 'pi', r'\\\\rho': 'rho', r'\\\\sigma': 'sigma',\n",
    "        r'\\\\tau': 'tau', r'\\\\upsilon': 'upsilon', r'\\\\phi': 'phi',\n",
    "        r'\\\\chi': 'chi', r'\\\\psi': 'psi', r'\\\\omega': 'omega',\n",
    "        # Trigonometric functions\n",
    "        r'\\\\sin': 'sine', r'\\\\cos': 'cosine', r'\\\\tan': 'tangent',\n",
    "        r'\\\\csc': 'cosecant', r'\\\\sec': 'secant', r'\\\\cot': 'cotangent',\n",
    "        # Differential and partial differential\n",
    "        r'\\\\partial': 'partial', r'\\\\nabla': 'nabla',\n",
    "        r'\\\\mathrm\\{d\\}': 'd',  # For derivatives\n",
    "        # Other mathematical symbols\n",
    "        r'\\\\times': 'times', r'\\\\div': 'divided by', r'\\\\cdot': 'dot',\n",
    "        # Additional symbols and operations\n",
    "        r'\\+': 'plus', r'\\-': 'minus', r'\\*': 'times',\n",
    "        # Handling general exponentiation\n",
    "        r'\\\\exp\\{(.*?)\\}': r'e to the power of \\1',  # For exponential functions\n",
    "        r'(\\w+)\\^(\\w+)': r'\\1 to the power of \\2',  # General exponentiation\n",
    "        # Handling \\mathop\n",
    "        r'\\\\mathop\\{\\\\rm ([^}]+)\\}': r'operator \\1'    }\n",
    "    \n",
    "    # Function to apply replacements to a matched object\n",
    "    def apply_replacements(match):\n",
    "        # Extracting the matched text excluding the [math] tags\n",
    "        math_text = match.group(1) # match.group(0) includes the whole match, so match.group(1) is the first capture group\n",
    "        \n",
    "        # Applying all replacements to the math_text\n",
    "        for pattern, replacement in replacements.items():\n",
    "            math_text = re.sub(pattern, replacement, math_text)\n",
    "        \n",
    "        # Return the transformed math_text\n",
    "        return math_text\n",
    "\n",
    "    # Use=ing re.sub with a function that applies the replacements for each [math] section\n",
    "    # Pattern captures the content between [math] and [/math] tags\n",
    "    pattern = r'\\[math\\](.*?)\\[/math\\]'\n",
    "    clean_text = re.sub(pattern, apply_replacements, text)\n",
    "\n",
    "    # Removing unnecessary braces and cleanup, applied globally to the whole text\n",
    "    clean_text = re.sub(r'\\{|\\}', '', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    #handling LaTex expressions\n",
    "    text = clean_math_text(text)\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Removing HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Expanding contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Removing special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Removing extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # removing stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being non-duplicate:  0.3640749710596458\n",
      "Probability of being duplicate:  0.6359250289403542\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text):\n",
    "    # Contractions map\n",
    "    contractions_dict = {\n",
    "        \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "        }\n",
    "    # Regular expression for finding contractions\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    def replace(match): return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "def generate_features(clean_q1, clean_q2):\n",
    "    # Calculate lengths of each question\n",
    "    lengthq1 = len(clean_q1)\n",
    "    lengthq2 = len(clean_q2)\n",
    "    \n",
    "    # Calculate word lengths of each question\n",
    "    q1_wordlen = len(clean_q1.split())\n",
    "    q2_wordlen = len(clean_q2.split())\n",
    "    \n",
    "    # Calculate common words\n",
    "    q1_words = set(clean_q1.split())\n",
    "    q2_words = set(clean_q2.split())\n",
    "    common_words = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Calculate word difference\n",
    "    word_difference = abs(q1_wordlen - q2_wordlen)\n",
    "    \n",
    "    return np.array([lengthq1, lengthq2, common_words, q1_wordlen, q2_wordlen, word_difference])\n",
    "\n",
    "def vectorize_and_reduce(question1, question2):\n",
    "    # Vectorize the questions\n",
    "    tfidf_q1 = tfidf_vectorizer.transform([question1])\n",
    "    tfidf_q2 = tfidf_vectorizer.transform([question2])\n",
    "    # Reduce dimensions\n",
    "    reduced_q1 = svd_model.transform(tfidf_q1)\n",
    "    reduced_q2 = svd_model.transform(tfidf_q2)\n",
    "    # Calculate squared differences\n",
    "    squared_differences = np.square(reduced_q1 - reduced_q2).flatten()\n",
    "    return squared_differences\n",
    "\n",
    "def predict_duplicate_proba(question1, question2):\n",
    "    # Clean the input questions\n",
    "    clean_q1 = clean_text(question1)\n",
    "    clean_q2 = clean_text(question2)\n",
    "    \n",
    "    # Generate all required features\n",
    "    features = generate_features(clean_q1, clean_q2)\n",
    "    \n",
    "    # Vectorize and reduce the cleaned questions\n",
    "    vector_features = vectorize_and_reduce(clean_q1, clean_q2)\n",
    "    \n",
    "    # Combine all features for prediction\n",
    "    final_features_array = np.hstack((features, vector_features))\n",
    "    \n",
    "    # Define feature names for the DataFrame\n",
    "    basic_feature_names = ['lengthq1', 'lengthq2', 'common_words', 'q1_wordlen', 'q2_wordlen', 'word_difference']\n",
    "    svd_feature_names = [str(i) for i in range(vector_features.shape[0])]  # SVD feature names as '0', '1', '2', ...\n",
    "    feature_names = basic_feature_names + svd_feature_names\n",
    "    \n",
    "    # Convert the final features array to a DataFrame with feature names\n",
    "    final_features_df = pd.DataFrame([final_features_array], columns=feature_names)\n",
    "    \n",
    "    # Predict probabilities using the GradientBoosting model\n",
    "    probas = gb_model.predict_proba(final_features_df)\n",
    "    return probas\n",
    "\n",
    "# Example usage\n",
    "question1 = \"What is the best thing that someone did for you on your birthday?\"\n",
    "question2 = \"What is the best thing someone did for you on your birthday?\"\n",
    "probas = predict_duplicate_proba(question1, question2)\n",
    "print(\"Probability of being non-duplicate: \", probas[0][0])\n",
    "print(\"Probability of being duplicate: \", probas[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5652173913043478\n",
      "Recall: 0.40625\n",
      "Accuracy: 0.71\n",
      "F1 Score: 0.4727272727272727\n",
      "F-beta Score: 0.5241935483870969\n",
      "\n",
      "Threshold: 0.2\n",
      "Precision: 0.48333333333333334\n",
      "Recall: 0.90625\n",
      "Accuracy: 0.66\n",
      "F-beta Score: 0.5330882352941178\n",
      "\n",
      "Threshold: 0.25\n",
      "Precision: 0.5490196078431373\n",
      "Recall: 0.875\n",
      "Accuracy: 0.73\n",
      "F-beta Score: 0.5932203389830509\n",
      "\n",
      "Threshold: 0.3\n",
      "Precision: 0.5952380952380952\n",
      "Recall: 0.78125\n",
      "Accuracy: 0.76\n",
      "F-beta Score: 0.625\n",
      "\n",
      "Threshold: 0.35\n",
      "Precision: 0.5675675675675675\n",
      "Recall: 0.65625\n",
      "Accuracy: 0.73\n",
      "F-beta Score: 0.5833333333333334\n",
      "\n",
      "Threshold: 0.39999999999999997\n",
      "Precision: 0.5555555555555556\n",
      "Recall: 0.625\n",
      "Accuracy: 0.72\n",
      "F-beta Score: 0.5681818181818182\n",
      "\n",
      "Threshold: 0.44999999999999996\n",
      "Precision: 0.5862068965517241\n",
      "Recall: 0.53125\n",
      "Accuracy: 0.73\n",
      "F-beta Score: 0.5743243243243242\n",
      "\n",
      "Threshold: 0.49999999999999994\n",
      "Precision: 0.5652173913043478\n",
      "Recall: 0.40625\n",
      "Accuracy: 0.71\n",
      "F-beta Score: 0.5241935483870969\n",
      "\n",
      "Threshold: 0.5499999999999999\n",
      "Precision: 0.5789473684210527\n",
      "Recall: 0.34375\n",
      "Accuracy: 0.71\n",
      "F-beta Score: 0.5092592592592592\n",
      "\n",
      "Threshold: 0.5999999999999999\n",
      "Precision: 0.6\n",
      "Recall: 0.28125\n",
      "Accuracy: 0.71\n",
      "F-beta Score: 0.4891304347826086\n",
      "\n",
      "Threshold: 0.6499999999999999\n",
      "Precision: 0.7\n",
      "Recall: 0.21875\n",
      "Accuracy: 0.72\n",
      "F-beta Score: 0.48611111111111105\n",
      "\n",
      "Threshold: 0.7\n",
      "Precision: 0.8\n",
      "Recall: 0.125\n",
      "Accuracy: 0.71\n",
      "F-beta Score: 0.3846153846153846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Assuming predict_duplicate_proba is already defined as per previous discussions\n",
    "\n",
    "df = pd.read_csv('questions.csv')\n",
    "df = df.sample(100, random_state=42)  # set random state for reproducibility\n",
    "probas = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    proba = predict_duplicate_proba(row['question1'], row['question2'])[0][1]\n",
    "    probas.append(proba)\n",
    "    y_true.append(row['is_duplicate'])\n",
    "    y_pred.append(1 if proba > 0.5 else 0)  # assuming 0.5 as the initial threshold\n",
    "\n",
    "# Function to calculate F-beta score\n",
    "def fbeta_score(precision, recall, beta=0.5):\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "# Calculate and print the precision, recall, accuracy, f1 score, and F-beta score\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "fbeta = fbeta_score(precision, recall, beta=0.5)  # More weight on precision\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"F-beta Score: {fbeta}\")\n",
    "\n",
    "# Run a loop of predict proba threshold of 0.2 to 0.7 with steps of 0.05 in between and check the metrics for all\n",
    "thresholds = np.arange(0.2, 0.75, 0.05)\n",
    "for threshold in thresholds:\n",
    "    y_pred = [1 if proba > threshold else 0 for proba in probas]\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    fbeta = fbeta_score(precision, recall, beta=0.5)  # Calculate F-beta score for each threshold\n",
    "\n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F-beta Score: {fbeta}\")  # Print F-beta score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Threshold | Precision | Recall | Accuracy | F-beta Score |\n",
    "|-----------|-----------|--------|----------|--------------|\n",
    "| **0.3**   | 0.60      | 0.78   | 0.76     | **0.62**     |\n",
    "| 0.25      | 0.55      | 0.88   | 0.73     | 0.59         |\n",
    "| 0.35      | 0.57      | 0.66   | 0.73     | 0.58         |\n",
    "| 0.45      | 0.59      | 0.53   | 0.73     | 0.57         |\n",
    "| 0.4       | 0.56      | 0.62   | 0.72     | 0.57         |\n",
    "| 0.2       | 0.48      | 0.91   | 0.66     | 0.53         |\n",
    "| 0.5       | 0.57      | 0.41   | 0.71     | 0.52         |\n",
    "| 0.55      | 0.58      | 0.34   | 0.71     | 0.51         |\n",
    "| 0.6       | 0.60      | 0.28   | 0.71     | 0.49         |\n",
    "| 0.65      | 0.70      | 0.22   | 0.72     | 0.49         |\n",
    "| 0.7       | 0.80      | 0.13   | 0.71     | 0.38         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting $\\beta$ to 0.5 in the F-beta score formula, it emphasizes precision over recall. Specifically:\n",
    "\n",
    "- The weight for precision is inversely proportional to the square of $\\beta$.\n",
    "- The weight for recall is directly proportional to the square of $\\beta$.\n",
    "\n",
    "The F-beta score formula is:\n",
    "\n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) \\times \\frac{{\\text{precision} \\times \\text{recall}}}{{(\\beta^2 \\times \\text{precision}) + \\text{recall}}}\n",
    "$$\n",
    "\n",
    "With $\\beta$ set to 0.5, the formula becomes:\n",
    "\n",
    "$$\n",
    "F_{0.5} = (1 + 0.5^2) \\times \\frac{{\\text{precision} \\times \\text{recall}}}{{(0.5^2 \\times \\text{precision}) + \\text{recall}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{0.5} = 1.25 \\times \\frac{{\\text{precision} \\times \\text{recall}}}{{0.25 \\times \\text{precision} + \\text{recall}}}\n",
    "$$\n",
    "\n",
    "This implies that precision is weighted by the factor $\\frac{1}{0.5^2} = \\frac{1}{0.25} = 4$ times more than recall in the calculation of the $F_{0.5}$ score. Therefore, in this case, precision is given 4 times the importance of recall. This weighting scheme is particularly useful when false positives are more costly or undesirable than false negatives.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enterpriseml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
