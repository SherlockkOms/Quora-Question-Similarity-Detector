{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omsan\\anaconda3\\envs\\enterpriseml\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\omsan\\AppData\\Local\\Temp\\ipykernel_15896\\4224451230.py:52: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  return results.groupby('Model').mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.589698</td>\n",
       "      <td>0.602802</td>\n",
       "      <td>0.577365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.524799</td>\n",
       "      <td>0.466831</td>\n",
       "      <td>0.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.668625</td>\n",
       "      <td>0.539385</td>\n",
       "      <td>0.556679</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.685625</td>\n",
       "      <td>0.582382</td>\n",
       "      <td>0.572579</td>\n",
       "      <td>0.593243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaiveBayes</th>\n",
       "      <td>0.575750</td>\n",
       "      <td>0.588404</td>\n",
       "      <td>0.461285</td>\n",
       "      <td>0.815203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.684750</td>\n",
       "      <td>0.548456</td>\n",
       "      <td>0.584668</td>\n",
       "      <td>0.520608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy        F1  Precision    Recall\n",
       "Model                                                      \n",
       "GradientBoosting    0.703000  0.589698   0.602802  0.577365\n",
       "KNN                 0.593000  0.524799   0.466831  0.612500\n",
       "LogisticRegression  0.668625  0.539385   0.556679  0.525000\n",
       "MLP                 0.685625  0.582382   0.572579  0.593243\n",
       "NaiveBayes          0.575750  0.588404   0.461285  0.815203\n",
       "SVC                 0.684750  0.548456   0.584668  0.520608"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['id', 'qid1', 'qid2', 'is_duplicate', 'clean_question1', 'clean_question2'])\n",
    "    y = data['is_duplicate']\n",
    "    return X, y\n",
    "\n",
    "def train_model(X_train, y_train, model_pipeline):\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    return model_pipeline\n",
    "\n",
    "def evaluate_model(model_pipeline, X_test, y_test):\n",
    "    predictions = model_pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def run_model_pipeline(data_paths):\n",
    "    results = pd.DataFrame(columns=['Dataset', 'Model', 'Accuracy', 'F1', 'Precision', 'Recall'])\n",
    "    top_models = {\n",
    "        'SVC': Pipeline([('scaler', StandardScaler()), ('SVC', SVC())]),\n",
    "        'LogisticRegression': Pipeline([('scaler', StandardScaler()), ('LogisticRegression', LogisticRegression())]),\n",
    "        'NaiveBayes': Pipeline([('scaler', StandardScaler()), ('NaiveBayes', GaussianNB())]),\n",
    "        'KNN': Pipeline([('scaler', StandardScaler()), ('KNN', KNeighborsClassifier())]),\n",
    "        'MLP': Pipeline([('scaler', StandardScaler()), ('MLP', MLPClassifier())]),\n",
    "        'GradientBoosting': Pipeline([('scaler', StandardScaler()), ('GradientBoosting', GradientBoostingClassifier())]),\n",
    "    }\n",
    "\n",
    "    for model_choice in top_models.keys():\n",
    "        for data_path in data_paths:\n",
    "            X, y = load_data(data_path)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            model_pipeline = top_models[model_choice]\n",
    "            trained_model = train_model(X_train, y_train, model_pipeline)\n",
    "            accuracy, f1, precision, recall = evaluate_model(trained_model, X_test, y_test)\n",
    "            new_result = pd.DataFrame({'Dataset': [data_path], 'Model': [model_choice], 'Accuracy': [accuracy], 'F1': [f1], 'Precision': [precision], 'Recall': [recall]})\n",
    "            results = pd.concat([results, new_result], ignore_index=True)\n",
    "\n",
    "    return results.groupby('Model').mean()\n",
    "\n",
    "# Example usage\n",
    "data_paths = ['nltk_embeddings.csv', 'spacy_embeddings.csv', 'nltk_embeddings_bert.csv', 'spacy_embeddings_bert.csv']\n",
    "run_model_pipeline(data_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We oberve that the top 3 models are GradientBoosting, MLP Classifier and SVC. So we would use grid search to perform hyperparamter  optimizaion of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omsan\\anaconda3\\envs\\enterpriseml\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Model: SVC\n",
      "Best Score: 0.681875\n",
      "Best Parameters: {'SVC__C': 10, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\n",
      "Model: MLP\n",
      "Best Score: 0.701875\n",
      "Best Parameters: {'MLP__activation': 'relu', 'MLP__hidden_layer_sizes': (100,), 'MLP__learning_rate_init': 0.001, 'MLP__solver': 'sgd'}\n",
      "\n",
      "Model: GradientBoosting\n",
      "Best Score: 0.729\n",
      "Best Parameters: {'GradientBoosting__learning_rate': 0.1, 'GradientBoosting__max_depth': 5, 'GradientBoosting__n_estimators': 200}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define pipelines for each model with StandardScaler and the model\n",
    "pipelines = {\n",
    "    'SVC': Pipeline([('scaler', StandardScaler()), ('SVC', SVC())]),\n",
    "    'MLP': Pipeline([('scaler', StandardScaler()), ('MLP', MLPClassifier(max_iter=300))]),\n",
    "    'GradientBoosting': Pipeline([('scaler', StandardScaler()), ('GradientBoosting', GradientBoostingClassifier())]),\n",
    "}\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'SVC': {\n",
    "        'SVC__C': [0.1, 1, 10],\n",
    "        'SVC__kernel': ['linear', 'rbf'],\n",
    "        'SVC__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'MLP': {\n",
    "        'MLP__hidden_layer_sizes': [(50,), (100,), (50,50)],\n",
    "        'MLP__activation': ['relu', 'tanh'],\n",
    "        'MLP__solver': ['adam', 'sgd'],\n",
    "        'MLP__learning_rate_init': [0.001, 0.01],\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'GradientBoosting__n_estimators': [100, 200],\n",
    "        'GradientBoosting__learning_rate': [0.01, 0.1],\n",
    "        'GradientBoosting__max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Results dictionary\n",
    "tuning_results = {}\n",
    "\n",
    "# Load data and split it\n",
    "X, y = load_data(data_paths[0])  # using the first dataset for hyperparameter tuning\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    tuning_results[model_name] = {'Best Model': best_model, 'Best Params': best_params, 'Best Score': best_score}\n",
    "\n",
    "# Display fine-tuning results\n",
    "for model_name, results in tuning_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Score: {results['Best Score']}\")\n",
    "    print(f\"Best Parameters: {results['Best Params']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T20:37:04.622839Z",
     "start_time": "2024-02-11T20:36:14.504651Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on file nltk_embeddings.csv\n",
      "Training model SVC with best parameters\n",
      "Results for model SVC on file nltk_embeddings.csv:\n",
      "Accuracy: 0.6845\n",
      "F1 Score: 0.6847175264908864\n",
      "Precision: 0.6849436081178641\n",
      "Recall: 0.6845\n",
      "\n",
      "Training model MLP with best parameters\n",
      "Results for model MLP on file nltk_embeddings.csv:\n",
      "Accuracy: 0.6775\n",
      "F1 Score: 0.6826586487799645\n",
      "Precision: 0.7236948607868243\n",
      "Recall: 0.6775\n",
      "\n",
      "Training model GradientBoosting with best parameters\n",
      "Results for model GradientBoosting on file nltk_embeddings.csv:\n",
      "Accuracy: 0.7135\n",
      "F1 Score: 0.7127953344500032\n",
      "Precision: 0.7121952601132279\n",
      "Recall: 0.7135\n",
      "\n",
      "Running models on file spacy_embeddings.csv\n",
      "Training model SVC with best parameters\n",
      "Results for model SVC on file spacy_embeddings.csv:\n",
      "Accuracy: 0.682\n",
      "F1 Score: 0.6820882841918752\n",
      "Precision: 0.6821779318560673\n",
      "Recall: 0.682\n",
      "\n",
      "Training model MLP with best parameters\n",
      "Results for model MLP on file spacy_embeddings.csv:\n",
      "Accuracy: 0.6665\n",
      "F1 Score: 0.6721961425046898\n",
      "Precision: 0.7033046850539806\n",
      "Recall: 0.6665\n",
      "\n",
      "Training model GradientBoosting with best parameters\n",
      "Results for model GradientBoosting on file spacy_embeddings.csv:\n",
      "Accuracy: 0.715\n",
      "F1 Score: 0.714083075119879\n",
      "Precision: 0.7133420100035199\n",
      "Recall: 0.715\n",
      "\n",
      "Running models on file nltk_embeddings_bert.csv\n",
      "Training model SVC with best parameters\n",
      "Results for model SVC on file nltk_embeddings_bert.csv:\n",
      "Accuracy: 0.6565\n",
      "F1 Score: 0.6600141615777317\n",
      "Precision: 0.666200234909243\n",
      "Recall: 0.6565\n",
      "\n",
      "Training model MLP with best parameters\n",
      "Results for model MLP on file nltk_embeddings_bert.csv:\n",
      "Accuracy: 0.6995\n",
      "F1 Score: 0.7023614719859198\n",
      "Precision: 0.707613327951389\n",
      "Recall: 0.6995\n",
      "\n",
      "Training model GradientBoosting with best parameters\n",
      "Results for model GradientBoosting on file nltk_embeddings_bert.csv:\n",
      "Accuracy: 0.702\n",
      "F1 Score: 0.697840835567058\n",
      "Precision: 0.696234756097561\n",
      "Recall: 0.702\n",
      "\n",
      "Running models on file spacy_embeddings_bert.csv\n",
      "Training model SVC with best parameters\n",
      "Results for model SVC on file spacy_embeddings_bert.csv:\n",
      "Accuracy: 0.6585\n",
      "F1 Score: 0.6623335966991026\n",
      "Precision: 0.6696005130460726\n",
      "Recall: 0.6585\n",
      "\n",
      "Training model MLP with best parameters\n",
      "Results for model MLP on file spacy_embeddings_bert.csv:\n",
      "Accuracy: 0.688\n",
      "F1 Score: 0.6835240164238581\n",
      "Precision: 0.681654466058681\n",
      "Recall: 0.688\n",
      "\n",
      "Training model GradientBoosting with best parameters\n",
      "Results for model GradientBoosting on file spacy_embeddings_bert.csv:\n",
      "Accuracy: 0.7145\n",
      "F1 Score: 0.7119394252252003\n",
      "Precision: 0.7105709768974476\n",
      "Recall: 0.7145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    'nltk_embeddings.csv',\n",
    "    'spacy_embeddings.csv',\n",
    "    'nltk_embeddings_bert.csv',\n",
    "    'spacy_embeddings_bert.csv',\n",
    "]\n",
    "\n",
    "# Define a function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Loop over each file\n",
    "for file in files:\n",
    "    print(f\"Running models on file {file}\")\n",
    "    \n",
    "    # Load data and split it\n",
    "    X, y = load_data(file)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Loop over each model in tuning results\n",
    "    for model_name, model_info in tuning_results.items():\n",
    "        print(f\"Training model {model_name} with best parameters\")\n",
    "        \n",
    "        # Get the best parameters from tuning results\n",
    "        best_params = model_info['Best Params']\n",
    "        \n",
    "        # Remove the model name prefix from the best parameters\n",
    "        best_params = {k.replace(model_name+'__', ''): v for k, v in best_params.items()}\n",
    "        \n",
    "        # Initialize the model with the best parameters\n",
    "        if model_name == 'SVC':\n",
    "            model = SVC(**best_params)\n",
    "        elif model_name == 'MLP':\n",
    "            model = MLPClassifier(**best_params)\n",
    "        elif model_name == 'GradientBoosting':\n",
    "            model = GradientBoostingClassifier(**best_params)\n",
    "        \n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        accuracy, f1, precision, recall = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"Results for model {model_name} on file {file}:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T21:10:08.195440Z",
     "start_time": "2024-02-11T20:42:06.666304Z"
    },
    "collapsed": false
   },
   "source": [
    "| Model Name        | Dataset Name            | Accuracy | F1 Score   | Precision  | Recall |\n",
    "|-------------------|-------------------------|----------|------------|------------|--------|\n",
    "| SVC               | nltk_embeddings.csv     | 0.6845   | 0.6847     | 0.6849     | 0.6845 |\n",
    "| MLP               | nltk_embeddings.csv     | 0.6775   | 0.6827     | 0.7237     | 0.6775 |\n",
    "| GradientBoosting  | nltk_embeddings.csv     | 0.7135   | 0.7128     | 0.7122     | 0.7135 |\n",
    "| SVC               | spacy_embeddings.csv    | 0.682    | 0.6821     | 0.6822     | 0.682  |\n",
    "| MLP               | spacy_embeddings.csv    | 0.6665   | 0.6722     | 0.7033     | 0.6665 |\n",
    "| GradientBoosting  | spacy_embeddings.csv    | 0.715    | 0.7141     | 0.7133     | 0.715  |\n",
    "| SVC               | nltk_embeddings_bert.csv| 0.6565   | 0.6600     | 0.6662     | 0.6565 |\n",
    "| MLP               | nltk_embeddings_bert.csv| 0.6995   | 0.7024     | 0.7076     | 0.6995 |\n",
    "| GradientBoosting  | nltk_embeddings_bert.csv| 0.702    | 0.6978     | 0.6962     | 0.702  |\n",
    "| SVC               | spacy_embeddings_bert.csv| 0.6585  | 0.6623     | 0.6696     | 0.6585 |\n",
    "| MLP               | spacy_embeddings_bert.csv| 0.688   | 0.6835     | 0.6817     | 0.688  |\n",
    "| GradientBoosting  | spacy_embeddings_bert.csv| 0.7145  | 0.7119     | 0.7106     | 0.7145 |\n",
    "\n",
    "\n",
    "We can observe that the GradientBoosting Cassifier is working the best with the spacy embedings, it is the highest f1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gbmodel.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Saving the GradientBoosting model trained on the spacy_embeddings.csv dataset into a joblib file but now on the entire dataset\n",
    "from joblib import dump\n",
    "\n",
    "# Load data\n",
    "X, y = load_data('spacy_embeddings.csv')\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "best_params = tuning_results['GradientBoosting']['Best Params']\n",
    "best_params = {k.replace(model_name+'__', ''): v for k, v in best_params.items()}\n",
    "model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save the model to a file\n",
    "dump(model, 'gbmodel.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
